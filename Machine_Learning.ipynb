{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/P+ttGkWiJDOTSUUFWRUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandondiazlopez/Small_ML_Project/blob/main/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brandon Diaz-Lopez\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mwMJ-lt0rmc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using Cross Validation\n",
        "from pandas import read_csv\n",
        "from sklearn . model_selection import RepeatedKFold\n",
        "from sklearn . model_selection import KFold\n",
        "from sklearn . model_selection import cross_val_score\n",
        "from sklearn . discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn . linear_model import LogisticRegression\n",
        "from sklearn . neighbors import KNeighborsClassifier\n",
        "from sklearn . naive_bayes import GaussianNB\n",
        "from sklearn . svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read and load the csv data file\n",
        "filename = \"myClassDataSet2.csv\"\n",
        "data = read_csv( filename )"
      ],
      "metadata": {
        "id": "sw8u3M-tskwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section was used to set up the proper libraries needed for the evaluation."
      ],
      "metadata": {
        "id": "QwLfHWsCoqjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate array into input and output components\n",
        "array = data.values\n",
        "\n",
        "X = array[:,0:10]\n",
        "Y = array[:,10]\n",
        "\n",
        "# Create RepeatedKFold object\n",
        "rkf = RepeatedKFold(n_splits=10, n_repeats=3, random_state=4)\n",
        "\n",
        "# Create logistic regression model with liblinear solver\n",
        "logreg = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Perform cross-validation and compute accuracy\n",
        "logreg.fit(X,Y)\n",
        "accuracy = cross_val_score(logreg, X, Y, cv=10, scoring='accuracy')\n",
        "\n",
        "# Compute and print mean accuracy\n",
        "mean_accuracy = sum(accuracy) / len(accuracy)\n",
        "print(\"Mean classification accuracy: \", mean_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpgtLMdbwhZD",
        "outputId": "3d796539-2205-42a5-f40e-565b1287e258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean classification accuracy:  0.9410000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section the data is converted to an array to make it easier to perform model functions such as Repeated K fold, Logistic Regression and Cross Validation Score. \n",
        "\n",
        "The 10 folds essentially is running the model 10 times, training the model each fold, with the exception of the last fold, which will be used for the test. \n",
        "\n",
        "Having a Mean Classification Score of 0.941 means that the model is 94.1% accurate. Meaning it is a very accurate model."
      ],
      "metadata": {
        "id": "HPGbUB5sgxBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reporting Accuracy, Neg Log Loss, and ROC AUC scores\n",
        "\n",
        "accuracy_scores = cross_val_score(logreg, X, Y, cv=10, scoring='accuracy')\n",
        "neglogloss_scores = cross_val_score(logreg, X, Y, cv=10, scoring='neg_log_loss')\n",
        "rocauc_scores = cross_val_score(logreg, X, Y, cv=10, scoring='roc_auc')\n",
        "\n",
        "print('Mean accuracy:', accuracy_scores.mean())\n",
        "print('Mean negative log loss:', -neglogloss_scores.mean())\n",
        "print('Mean ROC AUC:', rocauc_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5cUqkNTYkyW",
        "outputId": "28437841-15ef-4d12-e4be-df8ddd9e172b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.9410000000000001\n",
            "Mean negative log loss: 0.1969084652481124\n",
            "Mean ROC AUC: 0.9676273343359785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the Mean Accuracy score of 94.1 means that the model is extremely accurate.\n",
        "\n",
        " Having a  Mean Negative log loss score of 0.1969 means that there is a low chance for errors. The lower the score, the lower chance for error. \n",
        "\n",
        "For the Mean ROC AUC to be 0.9676 means that the model is nearly perfect. "
      ],
      "metadata": {
        "id": "PjxS-WLfg2Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing linear and nonlinear models\n",
        "\n",
        "models = []\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(gamma='scale')))\n",
        "\n",
        "#Evaluating each model\n",
        "results = []\n",
        "\n",
        "names = []\n",
        "\n",
        "for name, model in models: \n",
        "  kfold = KFold(n_splits=10, random_state=4, shuffle=True)\n",
        "  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(\"{}: mean accuracy:{}, \\t std of accuracies:{}\".format(name, cv_results.mean(), cv_results.std()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGicwMeHYvkX",
        "outputId": "537a081c-e259-4e3c-c036-4aadcb28c736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA: mean accuracy:0.9318000000000002, \t std of accuracies:0.005996665740225956\n",
            "LR: mean accuracy:0.9409000000000001, \t std of accuracies:0.0064412731660751415\n",
            "KNN: mean accuracy:0.9353, \t std of accuracies:0.008246817568007641\n",
            "NB: mean accuracy:0.9418, \t std of accuracies:0.005528109984434081\n",
            "SVM: mean accuracy:0.9434999999999999, \t std of accuracies:0.005590169943749448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this section, we are trying to compare the accuracy of our algorithm. To do this, we will use several different classifications to compare it to. \n",
        "\n",
        "Based off of these scores, I would choose to run my model though SVM or NB, because they have the highest accuacies and the lowest standard deviations."
      ],
      "metadata": {
        "id": "HxcRhXP_g4Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle( ' Algorithm Comparison ' )\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "1fJkaRJhY6IZ",
        "outputId": "5d90d07a-580c-4d4c-bbe7-163de455a8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the previous section, I would have chosen SVM and NB to run my model off of. HOWEVER, based on this graph, I would choose LDA because there are no outliers and the data appears to be more normally distributed than the LR, KNN, NB and SVM."
      ],
      "metadata": {
        "id": "PDJTZpIUg6pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline that extracts features from the data then creates a model\n",
        "from pandas import read_csv\n",
        "from sklearn . model_selection import KFold\n",
        "from sklearn . model_selection import cross_val_score\n",
        "from sklearn . pipeline import Pipeline\n",
        "from sklearn . pipeline import FeatureUnion\n",
        "from sklearn . linear_model import LogisticRegression\n",
        "from sklearn . decomposition import PCA\n",
        "from sklearn . feature_selection import SelectKBest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# read and load the csv data file\n",
        "filename = \"myClassDataSet2.csv\"\n",
        "dataframe = read_csv ( filename )\n",
        "array = dataframe . values\n",
        "\n",
        "# separate array into input and output components\n",
        "X = array [:,0:10]\n",
        "Y = array [:,10]"
      ],
      "metadata": {
        "id": "R4c2Z01NZV9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Standarize the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Step 2: Extract the 3 features using PCA\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# Step 3: Extract 6 features using SelectKBest\n",
        "k_best = SelectKBest(k=6)\n",
        "\n",
        "# Step 4: Combine features, extracted from two previous steps using FeatureUnion()\n",
        "feature_union = FeatureUnion([(\"pca\", pca), (\"k_best\", k_best)])\n",
        "\n",
        "# Step 5: Learn a Logistic Regression using ‘liblinear’ solver.\n",
        "logistic_regression = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Create Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', scaler),\n",
        "    ('features', feature_union),\n",
        "    ('logistic_regression', logistic_regression)\n",
        "])\n",
        "\n",
        "#Evaluate pipeline using Kfold Cross Val\n",
        "Kfold = KFold(random_state=5, shuffle=True)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(\"Accuarcy Score:\", results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdE45f87ahsx",
        "outputId": "d0f50b17-658e-4a29-b0e8-d4d1926d8e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuarcy Score: 0.9434999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are creating a pipeline to automate standard workflows. Standard workfolws help the model overcome problems like data leakage. \n",
        "\n",
        "We first will standardize the model to eliminate bias and to improve model performance. Then we will use PCA to extract the top 3 principle components. Once that is done SelectKBest will pick the best 6 features for the target variable.\n",
        "\n",
        "Feature union combines both of these exrtraction features. Then we will use Logistic Regression to make binary predicitons of our criteria.\n",
        "\n",
        "Having a score of 0.9434 means that the model is 94.34% accurate. This average accuracy means that the model is really effective."
      ],
      "metadata": {
        "id": "tLbPBN1yvqLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages to the Jupyter notebook\n",
        "# Bagged Decision Trees for Classification\n",
        "from pandas import read_csv\n",
        "from sklearn . model_selection import KFold\n",
        "from sklearn . model_selection import cross_val_score\n",
        "from sklearn . ensemble import BaggingClassifier\n",
        "from sklearn . ensemble import AdaBoostClassifier\n",
        "from sklearn . ensemble import VotingClassifier\n",
        "from sklearn . tree import DecisionTreeClassifier\n",
        "from sklearn . linear_model import LogisticRegression\n",
        "from sklearn . svm import SVC\n",
        "\n",
        "# read and load the csv data file\n",
        "filename = \"myClassDataSet2.csv\"\n",
        "dataframe = read_csv ( filename )\n",
        "array = dataframe . values\n",
        "\n",
        "# separate array into input and output components\n",
        "X = array [:,0:10]\n",
        "Y = array [:,10]"
      ],
      "metadata": {
        "id": "xQ-OUnUWapYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the Bagging Classifer \n",
        "\n",
        "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "num_trees = 100\n",
        "\n",
        "model = BaggingClassifier(base_estimator=tree, n_estimators=num_trees, random_state=7)\n",
        "\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "\n",
        "print(\"Accuracy Score:\", results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36DR8VoqbAyV",
        "outputId": "4424e88e-73c9-4f65-eef0-056e500c577d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.9494999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the goal is to boost the accuracy of our model. The Bagging Classifer takes multiple samples and trains those models for each sample. The accuracy score is the average of all of the samples pulled.\n",
        "\n",
        "Again since we have this high of a score, it means that the model is very accurate."
      ],
      "metadata": {
        "id": "TpfkS8vbyLC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost Classification\n",
        "\n",
        "adamodel = AdaBoostClassifier(n_estimators=num_trees, random_state=7)\n",
        "\n",
        "results = cross_val_score(adamodel, X, Y, cv=kfold)\n",
        "\n",
        "print(\"Accuracy Score:\", results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLcfOzEwbIa9",
        "outputId": "d9e2c919-64ff-48a7-9234-a5c2d0bd73b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.9454999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we are using AdaBoost to boost our algorithm. This classifcation essentially combines weaker classifers into a stronger one. Having a score of 0.9454 means that the model is still predicting at a very high level. "
      ],
      "metadata": {
        "id": "R-a-rytQzHBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Sub models\n",
        "\n",
        "estimators = []\n",
        "\n",
        "log_reg = LogisticRegression(solver ='liblinear')\n",
        "estimators.append (('logistic', log_reg))\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "estimators.append(('cart', dtc))\n",
        "\n",
        "supp_vec = SVC(gamma='auto')\n",
        "estimators.append(('svm', supp_vec))\n",
        "\n",
        "# Creating the ensemble model\n",
        "\n",
        "ensemble = VotingClassifier(estimators)\n",
        "\n",
        "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
        "print('mean estimate of classification accuracy', results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfakMgTibMl6",
        "outputId": "72a8f13d-f3b6-4eb4-8283-3aa3c2de273c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean estimate of classification accuracy 0.9468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finally, we are using the voting classifer. This classifer combines the predictions from several algorithms. The classifer needs to make a couple models outside of your sub models. Then it uses the sub models to help predict your new data. Having an accuracy of 0.9468 means that the model is still effective."
      ],
      "metadata": {
        "id": "VUEDw_uNzv2p"
      }
    }
  ]
}